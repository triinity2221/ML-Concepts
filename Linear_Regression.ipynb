{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regression is the process of predicting a continuous values.</h3>\n",
    "<p>\n",
    "<li> Simple Regression: Uses only one dependent variable </li> \n",
    "<li> Multiple Regression: Uses multiple dependent variable </li> </p>\n",
    "\n",
    "****\n",
    "<h4>Various regression algorithms:</h4>\n",
    "<ol> \n",
    "<li>Ordinal Regression</li>\n",
    "<li>Poisson Regression</li>\n",
    "<li>Fast forest quantile regression</li>\n",
    "<li>Linear, Polynomial, Lasso, Stepwise, Ridge regression</li>\n",
    "<li>Bayesian Linear regression</li>\n",
    "<li>Neural network regression</li>\n",
    "<li>Decission forest regression</li>\n",
    "<li>Boosted decission tree regression</li>\n",
    "<li>KNN (K-nearest neighbors)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Different error metrics ($y_i $ is actual $ \\hat{y_i}$ is predicted):\n",
    "- Mean absolute error $ \\text{MAE} = \\frac{\\sum_{i=1}^{n}|y_i-\\hat{y_i}|}{n} $\n",
    "- Mean Square error $ \\text{MSE} = \\frac{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}{n} $ : This is generally preferred as it enlarges the larger errors and penalizes it\n",
    "- Root Mean Square error $ \\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}{n}} $ : This is also preferred as this is of same dimension as $y_i$\n",
    "- Relative Absolute error $\\text{RAE} = \\frac{\\sum_{j=1}^{n}|y_j-\\hat{y_j}|}{\\sum_{j=1}^{n}|y_j-\\bar{y}|} $\n",
    "- Relative Square error $\\text{RSE} = \\frac{\\sum_{j=1}^{n}(y_j-\\hat{y_j})^2}{\\sum_{j=1}^{n}(y_j-\\bar{y})^2} $: This metric is used to calculate the $R^2(=1-\\text{RSE})$ which is again a popular error represents how close the data points are to the fitted line more the value of $R^2$ best is the fit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Deriving the formula for finding the solution of a simple linear regression\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "\\hat{y}&=\\theta_0+\\theta_1x\\\\\n",
    "E(\\text{Error})&=\\sum_{i=1}^{n}(\\hat{y}_i-y_i)^2\\hspace{2mm} .....\\hspace{1mm}\\text{to be minimised}\\\\\n",
    "\\frac{\\partial E}{\\partial \\theta_0} &= \\frac{\\partial \\sum_{i=1}^{n}(\\hat{y}_i-y_i)^2}{\\partial \\theta_0}\\\\\n",
    "\\frac{\\partial E}{\\partial \\theta_0} &= \\frac{\\partial \\sum_{i=1}^{n}(\\theta_0+\\theta_1x_1-y_i)^2}{\\partial \\theta_0}\\\\\n",
    "\\frac{\\partial E}{\\partial \\theta_0} &= 2\\sum_{i=1}^{n}(\\theta_0+\\theta_1x_i-y_i)=0\\\\\n",
    "\\sum_{i=1}^{n}\\theta_0&+\\sum_{i=1}^{n}\\theta_1x_i-\\sum_{i=1}^{n}y_i=0\\\\\n",
    "n\\theta_0&+\\theta_1\\sum_{i=1}^{n}x_i-\\sum_{i=1}^{n}y_i=0\\\\\n",
    "\\theta_0&=\\frac{\\sum_{i=1}^{n}y_i}{n}-\\frac{\\theta_1\\sum_{i=1}^{n}x_i}{n}\\\\\n",
    "\\theta_0&=\\bar{y}-\\theta_1\\bar{x}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similary if we minimize w.r.t to $\\theta_1$ we get,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{n}(x_iy_i-\\theta_0x_i-\\theta_1x_i^2)&=0\\\\\n",
    "\\sum_{i=1}^{n}(x_iy_i-(\\bar{y}-\\theta_1\\bar{x})x_i-\\theta_1x_i^2)&=0\\hspace{3mm}\\text{....from the prev eq}\\\\\n",
    "\\theta_1 = \\frac{\\sum_{i=1}^{n}(x_iy_i-\\bar{y}x_i)}{\\sum_{i=1}^{n}(x_i^2-\\bar{x}x_i)}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pros of Linear Regression:\n",
    "<ol><li>Fast</li><li>No parameter tuning</li><li>Easy to interpret</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ways to evaluate the model\n",
    "<ol><li>Divide the data to mutually exclusive set of Train and test usually in the ratio of 9:1, where the model's Out of sample accuracy is fairly represented by the accuracy on test set. But still there would be a possible dependedncy of accuracy on the way test and train may have been split.</li>\n",
    "<li>To avoid any accuracy variance occuring because of the test-train split we can use a K-fold approach of train and testing. Here the entire dataset will be divided into K-mutually exclusive equal chunks. Then there would be K number of iterations of training and testing where in each iteration one chunk of data will be used as test and rest as training. The finally accuracy is the average of all the iteration's accuracy.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85aec63f8376a5e7664cfb917bced4e0ef789778ec2ca5a341e03dcb090658bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
